services:
  llm-server:
    # Use AMD's pre-built official image (RECOMMENDED - fastest startup)
    image: rocm/pytorch:rocm6.4.2_ubuntu24.04_py3.12_pytorch_release_2.6.0
    container_name: homelab-llm-server
    working_dir: /app
    command: python main.py
    ports:
      - "8001:8000"

    # WSL GPU passthrough (device + required libraries)
    devices:
      - /dev/dxg:/dev/dxg

    volumes:
      # Mount application code
      - .:/app
      # Persistent SQLite database
      - ./data:/app/data
      # Hugging Face cache for models
      - hf-cache:/root/.cache/huggingface
      # WSL GPU libraries (CRITICAL for GPU detection in Docker)
      - /usr/lib/wsl/lib/libdxcore.so:/usr/lib/libdxcore.so
      - /opt/rocm/lib/libhsa-runtime64.so.1:/opt/rocm/lib/libhsa-runtime64.so.1

    environment:
      # ROCm environment variables
      # NOTE: HSA_OVERRIDE_GFX_VERSION not needed when using proper library mounts
      # The mounted libhsa-runtime64.so.1 handles GPU detection automatically
      - ROCM_PATH=/opt/rocm
      - HIP_VISIBLE_DEVICES=0
      # Python/PyTorch settings
      - PYTORCH_HIP_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128
      # Hugging Face settings
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/transformers
      # Database path
      - DATABASE_PATH=/app/data/models.db
      # Optional: Hugging Face token
      # - HF_TOKEN=your_token_here

    # Install Python dependencies on container start
    entrypoint: >
      sh -c "
      pip install --no-cache-dir fastapi==0.115.0 uvicorn[standard]==0.32.0 pydantic==2.9.2 sqlmodel==0.0.22 sqlalchemy==2.0.35 transformers>=4.40.0 accelerate>=0.30.0 psutil==6.1.0 python-multipart==0.0.12 &&
      python main.py
      "

    restart: unless-stopped
    security_opt:
      - seccomp:unconfined
    cap_add:
      - SYS_PTRACE
    ipc: host
    shm_size: 8G  # AMD recommended for ROCm
    stdin_open: true
    tty: true

volumes:
  hf-cache:
    driver: local
