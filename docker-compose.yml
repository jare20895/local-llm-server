services:
  llm-server:
    # Use AMD's pre-built official image (RECOMMENDED - fastest startup)
    image: rocm/pytorch:rocm6.4.2_ubuntu24.04_py3.12_pytorch_release_2.6.0
    container_name: homelab-llm-server
    working_dir: /app
    command: python main.py
    ports:
      - "8001:8000"

    # WSL GPU passthrough (device + required libraries)
    devices:
      - /dev/dxg:/dev/dxg

    volumes:
      # Mount application code
      - .:/app
      # Persistent SQLite database
      - ./data:/app/data
      # Hugging Face cache for models (bind mount for easy access)
      - ./hf-cache:/root/.cache/huggingface
      # Secondary cache on Z: drive (for large/infrequent models)
      - /mnt/z/llm-models-cache:/mnt/z/llm-models-cache
      # WSL GPU libraries (CRITICAL for GPU detection in Docker)
      - /usr/lib/wsl/lib/libdxcore.so:/usr/lib/libdxcore.so
      - /opt/rocm/lib/libhsa-runtime64.so.1:/opt/rocm/lib/libhsa-runtime64.so.1

    # Load environment variables from .env file
    env_file:
      - .env

    environment:
      # ROCm environment variables
      # NOTE: HSA_OVERRIDE_GFX_VERSION not needed when using proper library mounts
      # The mounted libhsa-runtime64.so.1 handles GPU detection automatically
      - ROCM_PATH=/opt/rocm
      - HIP_VISIBLE_DEVICES=0
      # Python/PyTorch settings
      - PYTORCH_HIP_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128
      # Hugging Face settings
      - HF_HOME=/root/.cache/huggingface
      - PRIMARY_CACHE_PATH=/root/.cache/huggingface
      - TRANSFORMERS_NO_ADVISORY_WARNINGS=1
      - DISABLE_FLASH_ATTENTION=1
      # Database path
      - DATABASE_PATH=/app/data/models.db
      # Optional: Hugging Face token
      # - HF_TOKEN=your_token_here

    # Install Python dependencies on container start
    entrypoint: >
      sh -c "
      pip install --no-cache-dir --root-user-action=ignore fastapi==0.115.0 uvicorn[standard]==0.32.0 pydantic==2.9.2 sqlmodel==0.0.22 sqlalchemy==2.0.35 transformers>=4.40.0 accelerate>=0.30.0 psutil==6.1.0 python-multipart==0.0.12 &&
      python main.py
      "

    restart: unless-stopped
    security_opt:
      - seccomp:unconfined
    cap_add:
      - SYS_PTRACE
    ipc: host
    shm_size: 8G  # AMD recommended for ROCm
    stdin_open: true
    tty: true

  llm-frontend:
    build:
      context: .
      dockerfile: vector-tester/Dockerfile
    container_name: vector-tester-frontend
    working_dir: /app
    environment:
      - LLM_API_BASE=http://llm-server:8000
      - LOG_DB_PATH=/app/data/vector-tester.db
      - PORT=3000
    volumes:
      - ./vector-tester/data:/app/data
    ports:
      - "4173:3000"
    depends_on:
      - llm-server
    restart: unless-stopped
